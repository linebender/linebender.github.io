<!doctype html>
<html lang="en">

    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <!-- Begin Jekyll SEO tag v2.8.0 -->
        <title>Towards fearless SIMD, 7 years later - Linebender</title>
        <meta name="generator" content="Zola v0.17.2">
        <meta property="og:title" content="Towards fearless SIMD, 7 years later">
        <meta property="og:locale" content="en_US">
        <meta name="description" content="Towards fearless SIMD, 7 years later">
        <meta property="og:description"
            content="Towards fearless SIMD, 7 years later">
        <meta property="og:site_name" content="Linebender">
        <meta property="og:type" content="website">
        <meta name="twitter:card" content="summary">
        <meta property="twitter:title" content="Towards fearless SIMD, 7 years later">
        <!-- End Jekyll SEO tag -->
        <link rel="stylesheet" href="/main.css">
        
        
        <link type="application/atom+xml" rel="alternate" href="/atom.xml" title="Linebender">
        
        
        <!-- mathjax support -->
        <script>
            MathJax = {
                tex: {
                    inlineMath: [['$', '$']]
                }
            };
        </script>
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    </head>

    <body cz-shortcut-listen="true">
        <header class="site-header" role="banner">
            <div class="wrapper">
                
                <a class="site-title" rel="author" href="/">Linebender</a>
                <nav class="site-nav">
                    <input type="checkbox" id="nav-trigger" class="nav-trigger">
                    <label for="nav-trigger" role="none">
                        <span class="menu-icon">
                            <svg viewBox="0 0 18 15" width="18px" height="15px">
                                <path
                                    d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z">
                                </path>
                            </svg>
                        </span>
                    </label>
                    <div class="trigger">
                        
                        <a class="page-link" href="/about">About</a>
                        <a class="page-link" href="/blog">Blog</a>
                        <a class="page-link" href="/wiki">Wiki</a>
                        
                    </div>
                </nav>
                
            </div>
        </header>
        <main class="page-content" aria-label="Content">
            <div class="wrapper">
                
<h1>Towards fearless SIMD, 7 years later</h1>
<h3>Raph Levien, March 29, 2025</h3>
<p>Seven years ago I wrote a blog post <a href="https://raphlinus.github.io/rust/simd/2018/10/19/fearless-simd.html">Towards fearless SIMD</a>, outlining a vision for Rust as a compelling language for writing fast SIMD programs.
Where are we now?</p>
<p>Unfortunately, the present-day experience of writing SIMD in Rust is still pretty rough, though there has been progress, and there are promising efforts underway.
As in the previous post, this post will outline a possible vision.</p>
<p>Up to now, Linebender projects have not used SIMD, but that is changing.
As we work on CPU/GPU hybrid rendering techniques, it's clear that we need SIMD to get maximal performance of the CPU side.
We also see opportunities in faster color conversion and accelerated 2D geometry primitives.</p>
<p>This blog post is also a companion to a <a href="https://www.youtube.com/watch?v=y0WcCUKxk50">podcast</a> I recorded recently with André Popovitch.
That podcast is a good introduction to SIMD concepts, while this blog post focuses more on future directions.</p>
<h2 id="a-simple-example">A simple example</h2>
<p>As a running example, we'll compute a <a href="https://raphlinus.github.io/audio/2018/09/05/sigmoid.html">sigmoid</a> function for a vector of 4 values.
The scalar version is as follows:</p>
<pre data-lang="rust" style="background-color:#2b303b;color:#c0c5ce;" class="language-rust "><code class="language-rust" data-lang="rust"><span style="color:#b48ead;">fn </span><span style="color:#8fa1b3;">sigmoid</span><span>(</span><span style="color:#bf616a;">x</span><span>: [</span><span style="color:#b48ead;">f32</span><span>; 4]) -&gt; [</span><span style="color:#b48ead;">f32</span><span>; </span><span style="color:#d08770;">4</span><span>] {
</span><span>    x.</span><span style="color:#96b5b4;">map</span><span>(|</span><span style="color:#bf616a;">y</span><span>| y / (</span><span style="color:#d08770;">1.0 </span><span>+ y * y).</span><span style="color:#96b5b4;">sqrt</span><span>())
</span><span>}
</span></code></pre>
<p>This particular simple code autovectorizes nicely (<a href="https://rust.godbolt.org/z/TfThE5r33">Godbolt link</a>), but more complex examples often fail to autovectorize, often because of subtle differences in floating point semantics.
(Editorial note: a previous version of this post didn't autovectorize (<a href="https://rust.godbolt.org/z/GoTEK3KT3">Godbolt</a>) because optimization level was set at <code>-O</code>, which is less aggressive than <code>-C opt-level=3</code>, the latter of which is the default for release builds)</p>
<h2 id="safety">Safety</h2>
<p>One of the biggest problems with writing SIMD in Rust is that all exposed SIMD intrinsics are marked as <code>unsafe</code>, even in cases where they can be used safely.
The reason is that support for SIMD features varies widely, and executing a SIMD instruction on a CPU that does not support it is undefined behavior – the chip can crash, ignore the instruction, or do something unexpected.
To be used safely, there must be some other mechanism to establish that the CPU does support the feature.</p>
<p>Here's the running example in hand-written intrinsic code, showing the need to write <code>unsafe</code> to access SIMD intrinsics at all:</p>
<pre data-lang="rust" style="background-color:#2b303b;color:#c0c5ce;" class="language-rust "><code class="language-rust" data-lang="rust"><span>#[</span><span style="color:#bf616a;">cfg</span><span>(target_arch = &quot;</span><span style="color:#a3be8c;">aarch64</span><span>&quot;)]
</span><span style="color:#b48ead;">fn </span><span style="color:#8fa1b3;">sigmoid_neon</span><span>(</span><span style="color:#bf616a;">x</span><span>: [</span><span style="color:#b48ead;">f32</span><span>; 4]) -&gt; [</span><span style="color:#b48ead;">f32</span><span>; </span><span style="color:#d08770;">4</span><span>] {
</span><span>    </span><span style="color:#b48ead;">use </span><span>core::arch::aarch64::*;
</span><span>    </span><span style="color:#b48ead;">unsafe </span><span>{
</span><span>        </span><span style="color:#b48ead;">let</span><span> x_simd = core::mem::transmute(x);
</span><span>        </span><span style="color:#b48ead;">let</span><span> x_squared = </span><span style="color:#96b5b4;">vmulq_f32</span><span>(x_simd, x_simd);
</span><span>        </span><span style="color:#b48ead;">let</span><span> ones = </span><span style="color:#96b5b4;">vdupq_n_f32</span><span>(</span><span style="color:#d08770;">1.0</span><span>);
</span><span>        </span><span style="color:#b48ead;">let</span><span> sum = </span><span style="color:#96b5b4;">vaddq_f32</span><span>(ones, x_squared);
</span><span>        </span><span style="color:#b48ead;">let</span><span> sqrt = </span><span style="color:#96b5b4;">vsqrtq_f32</span><span>(sum);
</span><span>        </span><span style="color:#b48ead;">let</span><span> ratio = </span><span style="color:#96b5b4;">vdivq_f32</span><span>(x_simd, sqrt);
</span><span>        core::mem::transmute(ratio)
</span><span>    }
</span><span>}
</span><span>
</span><span>#[</span><span style="color:#bf616a;">cfg</span><span>(target_arch = &quot;</span><span style="color:#a3be8c;">x86_64</span><span>&quot;)]
</span><span style="color:#b48ead;">fn </span><span style="color:#8fa1b3;">sigmoid_sse2</span><span>(</span><span style="color:#bf616a;">x</span><span>: [</span><span style="color:#b48ead;">f32</span><span>; 4]) -&gt; [</span><span style="color:#b48ead;">f32</span><span>; </span><span style="color:#d08770;">4</span><span>] {
</span><span>    </span><span style="color:#b48ead;">use </span><span>core::arch::x86_64::*;
</span><span>    </span><span style="color:#b48ead;">unsafe </span><span>{
</span><span>        </span><span style="color:#b48ead;">let</span><span> x_simd = core::mem::transmute(x);
</span><span>        </span><span style="color:#b48ead;">let</span><span> x_squared = </span><span style="color:#96b5b4;">_mm_mul_ps</span><span>(x_simd, x_simd);
</span><span>        </span><span style="color:#b48ead;">let</span><span> ones = </span><span style="color:#96b5b4;">_mm_set1_ps</span><span>(</span><span style="color:#d08770;">1.0</span><span>);
</span><span>        </span><span style="color:#b48ead;">let</span><span> sum = </span><span style="color:#96b5b4;">_mm_add_ps</span><span>(ones, x_squared);
</span><span>        </span><span style="color:#b48ead;">let</span><span> sqrt = </span><span style="color:#96b5b4;">_mm_sqrt_ps</span><span>(sum);
</span><span>        </span><span style="color:#b48ead;">let</span><span> ratio = </span><span style="color:#96b5b4;">_mm_div_ps</span><span>(x_simd, sqrt);
</span><span>        core::mem::transmute(ratio)
</span><span>    }
</span><span>}
</span></code></pre>
<p>This is quite a simplified example.
For one, the SIMD width is fixed at 4 lanes (128 bits).
Most likely, in practice you'd iterate over a larger slice, taking chunks equal to the natural SIMD width.</p>
<h2 id="multiversioning">Multiversioning</h2>
<p>A central problem important for SIMD is multiversioning and runtime dispatch.
In some cases, you know the exact CPU target, for example when compiling a binary you'll run only on your machine (in which case <code>target-cpu=native</code> is appropriate).
But when distributing software more widely, there may be a range of capabilities.
For highest performance, it's necessary to compile multiple versions of the code, and do runtime detection to dispatch to the best SIMD code the hardware can run.
This problem was expressed in the original fearless SIMD blog post, and there hasn't been significant advance at the Rust language level since then.</p>
<p>In the C++ world, the <a href="https://github.com/google/highway">Highway</a> library provides excellent SIMD support for a very wide range of targets, and also solves the multiversioning problem.
Among other uses are the codecs for the JPEG-XL image format.
Such codecs are an ideal use case for SIMD programming in general, and shipping them in a browser requires a good solution to multiversioning.
Highway has a really good explanation of <a href="https://github.com/kfjahnke/zimt/blob/multi_isa/examples/multi_isa_example/multi_simd_isa.md">their approach to multiversioning</a>.
It will be useful to study it carefully to see how they've solved various problems.
And a concise way of saying what I'd like to see is "Highway for Rust."</p>
<p>One possible approach is a crate called <a href="https://docs.rs/multiversion/latest/multiversion/">multiversion</a>, which uses macros to replicate the code for multiple versions.
A more recent macro-based approach is <a href="https://github.com/a4lg/rust-target-feature-dispatch">rust-target-feature-dispatch</a>.
It is generally a similar approach to multiversion, and the specific differences are set out in that crate's <a href="https://github.com/a4lg/rust-target-feature-dispatch/blob/main/src/README.md">README</a>.</p>
<p>Another approach, as I believe first advocated in my 2018 blog post, is to write functions polymorphic on a zero-sized type representing the SIMD capabilities, then rely on monomorphization to create the various versions.
One motivation for this approach is to encode safety in Rust's type system.
Having the zero-sized token is proof of the underlying CPU having a certain level of SIMD capability, so calling those intrinsics is safe.
A major library that uses this approach is <a href="https://docs.rs/pulp/latest/pulp/">pulp</a>, which also powers the <a href="https://docs.rs/faer/latest/faer/">faer</a> linear algebra library.</p>
<p>I started putting together a pulp version of the running example, but ran into the immediate problem that it lacks a <code>sqrt</code> intrinsic (this would be easy enough to add, however).
It also works a bit differently, in that it only supports vectors of the natural width, not ones of a fixed width.
For general linear algebra, that's fine, but for some other applications it adds friction, for example colors with alpha are naturally chunks of 4 scalars.
To see an example of pulp code, as well as some discussion, see this <a href="https://xi.zulipchat.com/#narrow/channel/255911-rust/topic/Rust.20SIMD.20thoughts/near/489370476">Zulip thread</a>.</p>
<p>In <a href="https://github.com/raphlinus/fearless_simd/pull/2">fearless_simd#2</a> I propose a prototype of reasonably-ergonomic SIMD multiversioning.
Like the original fearless_simd prototype, vector data types are polymorphic on SIMD level.
The new prototype goes beyond that in several important ways.
For one, arithmetic traits in std::ops are implemented for vector types, so it's possible to add two vectors together, multiply vectors by scalars, etc.</p>
<p>Here's what the running example looks like in that prototype:</p>
<pre data-lang="rust" style="background-color:#2b303b;color:#c0c5ce;" class="language-rust "><code class="language-rust" data-lang="rust"><span>#[</span><span style="color:#bf616a;">inline</span><span>(always)]
</span><span style="color:#b48ead;">fn </span><span style="color:#8fa1b3;">sigmoid_impl</span><span>&lt;S: Simd&gt;(</span><span style="color:#bf616a;">simd</span><span>: S, </span><span style="color:#bf616a;">x</span><span>: [</span><span style="color:#b48ead;">f32</span><span>; 4]) -&gt; [</span><span style="color:#b48ead;">f32</span><span>; </span><span style="color:#d08770;">4</span><span>] {
</span><span>    </span><span style="color:#b48ead;">let</span><span> x_simd: f32x4&lt;S&gt; = x.</span><span style="color:#96b5b4;">simd_into</span><span>(simd);
</span><span>    (x_simd / (</span><span style="color:#d08770;">1.0 </span><span>+ x_simd * x_simd).</span><span style="color:#96b5b4;">sqrt</span><span>()).</span><span style="color:#96b5b4;">into</span><span>()
</span><span>}
</span><span>
</span><span>simd_dispatch!(</span><span style="color:#96b5b4;">sigmoid</span><span>(level, rgba: [</span><span style="color:#b48ead;">f32</span><span>; </span><span style="color:#d08770;">4</span><span>]) -&gt; [</span><span style="color:#b48ead;">f32</span><span>; </span><span style="color:#d08770;">4</span><span>] = sigmoid_impl);
</span></code></pre>
<p>An advantage of the fearless_simd#2 prototype over pulp is a feature for downcasting based on SIMD level, so it's possible to write different code optimized for different chips.
See the <a href="https://github.com/raphlinus/fearless_simd/pull/2/files#diff-be8aece917a9235076ff8ec42749b1f1a803d2a3cbc2ccdd5425b405c74f7436">srgb example</a> in that pull request for more detail.
Though there are clear advantages, at this point I'm not sure whether this is the direction to go.
It would be a lot of work to build out all the needed types and operations, with potentially a large amount of repetitive boilerplate code in the library, which in turn may cause issues with compile time.
Another possible direction is a smarter, compiler-like proc macro which synthesizes the SIMD intrinsics as needed based on the types and operations in the source program.</p>
<p>One additional consideration for Rust is that the implementation of runtime feature detection is <a href="https://internals.rust-lang.org/t/better-codegen-for-cpu-feature-detection/22083">slower than it should be</a>.
Thus, feature detection and dispatch shouldn't be done at every function call.
A good working solution is to do feature detection once, at the start of the program, then pass that token down through function calls.
It's workable but definitely an ergonomic paper cut.</p>
<h2 id="fp16-and-avx-512">FP16 and AVX-512</h2>
<p>A general trend in parallel computation, really fueled by AI workloads, is smaller scalars with higher throughputs.
While not yet common on x86_64, the FP16 extension is supported on all Apple Silicon desktop CPUs and most recent high-ish end ARM-based phones.
Since Neon is only 128 bits wide, having 8 lanes is welcome.
I find the f16 format to be especially useful for pixel values, as it can encode color values with more than enough precision to avoid visual artifacts (8 bits is not quite enough, though it is good enough for some applications, as long as you're not trying to do HDR).</p>
<p>Native Rust support for the <code>f16</code> type has not yet landed (tracked in <a href="https://github.com/rust-lang/rust/issues/125440">rust#125440</a>), which makes use of this scalar size harder.
However, there is some support in the <a href="https://docs.rs/half/latest/half/">half</a> library, and also the <a href="https://github.com/raphlinus/fearless_simd/pull/2">fearless_simd#2</a> prototype exports a number of FP16 Neon instructions through inline assembly.
When true f16 support lands, it will be possible to switch over to intrinsics, which will have better optimization and ergonomics (for example, the same method will splat constants converted to <code>f16</code> at compile time and <code>f32</code> variables to be converted at runtime).</p>
<p>AVX-512 is a somewhat controversial SIMD capability.
It first appeared in the ill-fated Larrabee project, which shipped in limited numbers as the Xeon Phi starting in 2010, and has since appeared in scattered Intel CPUs, but with compromises.
In particular, sprinkling even a small amount of AVX-512 code into a program could result in downclocking, reducing performance for all workloads (see <a href="https://stackoverflow.com/questions/56852812/simd-instructions-lowering-cpu-frequency#comment100256395_56852812">Stack Overflow thread on throttling</a> for more details).
These days, the most likely way to get a CPU with AVX-512 is an AMD Zen 4 or Zen 5; it is on their strength that AVX-512 makes up about 16% of computers in the Steam hardware survey.</p>
<p>The increased width is not the main reason to be enthusiastic about AVX-512.
Indeed, on Zen 4 and most Zen 5 chips, the datapath is 256 bits so full 512 bit instructions are "double pumped." The most exciting aspect is predication based on masks, a common implementation technique on GPUs.
In particular, memory load and store operations are safe when the mask bit is zero, which is especially helpful for using SIMD efficiently on strings.
Without predication, a common technique is to write two loops, the first handling only even multiples of the SIMD width, and a second, usually written as scalars, to handle the odd-size "tail".
There are lots of problems with this - code bloat, worse branch prediction, inability to exploit SIMD for chunks slightly less than the natural SIMD width (which gets worse as SIMD grows wider), and risks that the two loops don't have exactly the same behavior.</p>
<p>Going forward, Intel has proposed AVX10, and will hopefully ship AVX 10.2 chips in the next few years.
This extension has pretty much all of the features of AVX-512, with some cleanups and new features (until recently, AVX10 was defined has having a 256 bit base width and optionally 512, but 512 is now the baseline).
In addition, AVX10.2 will include 16-bit floats (currently available only in the Sapphire Rapids high-end server and workstation chips).</p>
<h2 id="about-std-simd">About std::simd</h2>
<p>The "portable SIMD" work has been going on for many years and currently has a home as the nightly <a href="https://doc.rust-lang.org/std/simd/index.html">std::simd</a>.
While I think it will be very useful in many applications, I am not personally very excited about it for my applications.
For one, because it emphasizes portability, it encourages a "lowest common denominator" approach, while I believe that for certain use cases it will be important to tune algorithms to best use the specific quirks of the different SIMD implementations.
For two, std::simd does not itself solve the multiversion problem.
From my perspective, it's probably best to consider it as a souped-up version of autovectorization.</p>
<h2 id="language-evolution">Language evolution</h2>
<p>Rust's out of the box support for SIMD is still quite rough, especially the need to use <code>unsafe</code> extensively.
While some of the gap can be filled with libraries, arguably it should be a goal of the language itself to support safe SIMD code.
There is progress in this direction.</p>
<p>First, the original version of <code>target_feature</code> requires <code>unsafe</code> to call into <em>any</em> function annotated with <code>#[target_feature]</code>.
A proposal to relax that so that functions already under a target_feature gate can call safely call into another function with the same gate is called "<a href="https://rust-lang.github.io/rfcs/2396-target-feature-1.1.html">target_feature 1.1</a>" and is scheduled to ship in 1.86.
Closely related, once inside the suitable target_feature gate, the majority of SIMD intrinsics (broadly, those that don't do memory access through pointers) should be considered safe by the compiler, and that feature (safe intrinsics in core::arch) is also in flight.</p>
<p>There's more that can be done to help the Rust compiler recognize when SIMD use is safe, in particular to allow target_features when a concrete witness to the SIMD level is passed in as a function argument.
The "struct target_features" proposal (<a href="https://github.com/rust-lang/rfcs/pull/3525">RFC 3525</a>) enables target_feature in such cases, and is one of the proposals considered in the proposed Rust project goal <a href="https://rust-lang.github.io/rust-project-goals/2025h1/simd-multiversioning.html">Nightly support for ergonomic SIMD multiversioning</a>.</p>
<p>In general, improving Rust SIMD support will require both libraries and support in the Rust language.
Different approaches at the library level may indicate different language features to best support them.</p>
<h2 id="looking-forward">Looking forward</h2>
<p>My main goal in putting these prototypes forward, as well as writing these blog posts, is to spark conversation on how best to support SIMD programming in Rust.
If done well, it is a great opportunity for the language, and fits in with its focus on performance and portability.</p>
<p>As we build out the <a href="https://xi.zulipchat.com/#narrow/channel/197075-vello/topic/Potato.20-.20a.20paper.20design.20for.20a.20CPU.2FGPU.20hybrid.20renderer">Vello hybrid CPU/GPU renderer</a>, performance of the CPU components will rely heavily on SIMD, so we need to invest in writing a lot of SIMD code.
The most conservative approach would be hand-writing unsafe intrinsics-based code for all targets, but that's a lot of work and the use of unsafe is unappealing.
I'd love for the Rust ecosystem can come together and build good infrastructure, competitive with Highway.
For now, I think it's time to carefully consider the design space and try to come to consensus on what that should look like.</p>
<!-- TODO: not sure if this is the best link, we don't really have a project page for this -->


            </div>
        </main>
        <footer class="site-footer h-card">
            <div class="wrapper">
                
                <h2 class="footer-heading">Linebender</h2>
                <div class="footer-col-wrapper">
                    <div class="footer-col footer-col-1">
                        <ul class="contact-list">
                            <li class="p-name">Linebender</li>
                            <li><a class="u-email" href="mailto:raph@levien.com">raph@levien.com</a></li>
                        </ul>
                    </div>

                    <div class="footer-col footer-col-2">
                        <ul class="social-media-list">
                            <li><a href="https://github.com/linebender">
                                <svg class="svg-icon" role="none">
                                    <use xlink:href="/minima-social-icons.svg#github"></use>
                                </svg>
                                <span class="username">linebender</span>
                            </a></li>
                        </ul>
                    </div>

                    <div class="footer-col footer-col-3">
                        <p>&copy; 2023 The Linebender Authors.</p>
                        <p>
                            <a rel="license" href="http://creativecommons.org/licenses/by/4.0/">
                                <img alt="Creative Commons License" style="border-width:0"
                                    src="https://i.creativecommons.org/l/by/4.0/88x31.png" />
                            </a>
                            <br />
                            This website's content is licensed under a
                            <a rel="license" href="http://creativecommons.org/licenses/by/4.0/">
                                Creative Commons Attribution 4.0 International License
                            </a>.
                        </p>
                        <p>All code used in this website is available under the Apache-2.0 license.</p>
                    </div>
                </div>
                
            </div>
        </footer>
    </body>

</html>
